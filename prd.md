**ğŸ“˜ TruthScope â€“ Product Requirements Document (PRD)**
=======================================================

### _**AI vs Human Knowledge Verifier for Decentralized Trust Alignment**_

**Challenge:** Grokipedia vs Wikipedia Content Comparison (Challenge 1)

**ğŸ§  1. Executive Summary**
===========================

**TruthScope** is a decentralized, AI-powered truth verification system that compares **AI-generated content (Grokipedia)**with **human-curated content (Wikipedia)** and records discrepancies as **verifiable Community Notes** on the **OriginTrail Decentralized Knowledge Graph (DKG)**.

It is built on the updated **OriginTrail DKG (Nov 5)** and uses the **DKG Edge Node**, the **MCP agent layer**, and a **token-based trust and reputation layer** with optional **x402 micropayments** for trusted insights.

TruthScope transforms free-form encyclopedia entries into **structured RDF/JSON-LD Knowledge Assets**, detects hallucinations, biases, or contradictions, and stores the derived truth-alignment evidence immutably within the DKG.

**ğŸ¯ 2. Problem, Opportunity & Vision**
=======================================

### **ğŸ”¥ The Problem**

AI-generated encyclopedias like **Grokipedia** are powerful but opaque:

*   They may hallucinate facts
    
*   They may show ideological bias
    
*   Sources are inconsistent or missing
    
*   Their correctness cannot be independently verified
    
*   There is no accountability or provenance
    

Wikipedia, on the other hand, is human-curated but still fallible and inconsistent across revisions.

### **ğŸ’¡ Opportunity**

Using **OriginTrail DKG + AI + tokenomics**, we can:

*   Create a **neutral, decentralized trust layer** between competing knowledge bases
    
*   Make discrepancies machine-readable
    
*   Provide verifiable provenance for claims
    
*   Reward fact-verification
    
*   Expose high-confidence truth reports to both humans & AI agents
    

### **ğŸš€ Vision**

A world where **AI agents query truth like a protocol**, not an opinion.

TruthScope becomes the **canonical truth-alignment layer** between AI-generated content and the real world.

**ğŸ“ 3. Product Goals**
=======================

1.  **Automatically compare Grokipedia and Wikipedia content** for a fixed set of topics (via DKG Knowledge Assets or public APIs).
    
2.  Convert both pages into **structured RDF triples** stored as Knowledge Assets.
    
3.  Perform **semantic diffing** to detect hallucinations, bias, missing context, or contradictions.
    
4.  Generate **Community Note Knowledge Assets** for each discrepancy.
    
5.  Add a **token-backed confidence and staking model**.
    
6.  Offer **premium insights via x402 micropayments**.
    
7.  Provide an **MCP-driven TruthScope Agent** that uses the DKG for contextual truth when answering questions.
    
8.  Provide a **readable frontend** for browsing discrepancies.
    

**ğŸ§© 4. High-Level Architecture (Aligned with Agentâ€“Knowledgeâ€“Trust Layers)**
=============================================================================

Â Â Â Â Â Â Â Â Â Â Â Â Â Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â”‚ Â  Â  Â  Â  Â  USER / AI AGENTS Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â”‚Â  (Queries, UI, API consumers, MCP) Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â”‚

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â–¼

Â Â Â Â Â Â Â Â Â Â Â Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â Â Â Â Â Â Â Â Â Â Â Â â”‚ Â  Â  Â  Â  Â  Â  Â  AGENT LAYER Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â â”‚Â  Â  Â  Â  TruthScope MCP Agent Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â â”‚ - Fetch Grokipedia/Wikipedia contentÂ  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â â”‚ - Extract claims (LLM + symbolic reasoning) Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â â”‚ - Generate RDF triplesÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â â”‚ - Semantic diffingÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â â”‚ - Create/Query Community NotesÂ  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â Â Â Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â”‚

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â–¼

Â Â Â Â Â Â Â Â Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â Â Â Â Â Â Â Â Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  KNOWLEDGE LAYERÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  OriginTrail DKG (Nov 5) Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â â”‚Â  Â  KA1: Wikipedia triples Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â â”‚Â  Â  KA2: Grokipedia triplesÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â â”‚Â  Â  KA3: Community Notes Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â â”‚Â  Â  KA4: Provenance & staking metadata Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â Â â”‚Â  Â  (stored in JSON-LD & verifiable with Graph signatures) â”‚

Â Â Â Â Â Â Â Â Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â”‚

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â–¼

Â Â Â Â Â Â Â Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â Â Â Â Â Â Â Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  TRUST LAYERÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â â”‚Â  Â  - Token stakes (TRAC / NEURO) on claims Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â â”‚Â  Â  - Note reputation scoring Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â â”‚Â  Â  - x402 micropayments for premium reportsÂ  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â â”‚Â  Â  - Tamper-proof provenance via NeuroWeb parachainÂ  Â  Â  Â  Â  â”‚

Â Â Â Â Â Â Â Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**ğŸ”§ 5. Technical Components**
==============================

**5.1 Agent Layer â€“ â€œTruthScope Agentâ€**
----------------------------------------

**Built using:**

*   DKG Edge Node's MCP toolkit
    
*   LLM backend (OpenAI / Anthropic / Grok / local)
    
*   Python/Node.js orchestrator
    

### **Responsibilities:**

1.  **Fetch Content**
    
    *   Wikipedia REST API
        
    *   Grokipedia scraped HTML / Umanitek Knowledge Asset
        
    *   Cache the results as KA topics
        
2.  **Claim Extraction**
    
    *   Convert paragraphs to atomic factual claims
        
    *   Use LLM assisted extraction â†’ symbolic parse â†’ triple generation
        

Each claim becomes: Â Â 1.  **Semantic Diffing**
    
    *   Align claims between Wikipedia and Grokipedia
        
    *   Detect mismatches:
        
        *   Missing context
            
        *   Contradiction
            
        *   Bias framing
            
        *   Unsupported claims (hallucination)
            
        *   Citation absence
            
2.  **Community Note Generator**
    
    *   Summarizes discrepancy
        
    *   Creates JSON-LD note asset
        
    *   Publishes to DKG via Edge Node
        
3.  **Truth-Aware Q&A**
    
    *   When a user queries the TruthScope Agent:
        
        *   It first checks DKG notes related to topic
            
        *   Applies corrections before responding
            

This demonstrates **agentic reasoning + decentralized memory**.

**5.2 Knowledge Layer â€“ OriginTrail DKG (Nov 5)**
-------------------------------------------------

### **Knowledge Assets Used:**

#### **KA1: Wikipedia RDF Conversion**

*   Generated from Wikipedia wikitext â†’ JSON â†’ JSON-LD
    
*   Stored with schema:Article, schema:claim, prov:wasDerivedFrom
    

#### **KA2: Grokipedia RDF Conversion**

*   Scraped and cleaned
    
*   Converted into RDF claims
    
*   Stored with Grok provenance
    

#### **KA3: Community Notes**

Example JSON-LD structure:

{

Â Â "@context": "https://schema.org",

Â Â "@type": "AnalysisNote",

Â Â "topic": "Climate Change",

Â Â "sourceA": "Wikipedia",

Â Â "sourceB": "Grokipedia",

Â Â "finding": "Grokipedia minimizes the scientific consensus...",

Â Â "evidence": \["KA1:xyz", "KA2:abc"\],

Â Â "confidence": 0.92,

Â Â "stakedValue": "20 TRAC",

Â Â "createdAt": "2025-11-19T12:00:00Z"

}

#### **KA4: Token Staking Metadata**

*   DKG supports provenance & extensions
    
*   Store staking events and validators
    
*   Used for trust-weighting
    

**5.3 Trust Layer**
-------------------

### **Token staking:**

*   Fact-checkers stake **TRAC/NEURO** on their notes
    
*   Downvoted or proven wrong?â†’ stake slashed
    
*   Agreement from others â†’ stake grows
    
*   Higher staked assets = higher reliability score
    

### **Reputation model:**

ReputationScore = StakeWeight + AgreementScore + ProvenanceScore

### **x402 Micropayments:**

Premium:

*   Verified reports
    
*   Topic-level summaries
    
*   Truth-consensus scoreAccessible only via **x402 payment gate**.
    

**ğŸ§ª 6. Core Flows**
====================

**ğŸ” Flow 1: Topic Ingestion & Comparison**
-------------------------------------------

1.  User/agent chooses a topic: â€œClimate changeâ€
    
2.  TruthScope Agent fetches:
    
    *   Wikipedia content
        
    *   Grokipedia content
        
3.  LLM extracts claims
    
4.  Claims â†’ RDF triples
    
5.  Store KA1 / KA2
    
6.  Agent runs semantic-diff
    
7.  Generate discrepancy notes
    
8.  Publish KA3
    

Output:A set of trusted, verifiable Community Notes.

**ğŸ§  Flow 2: Truth-Aware Response Generation**
----------------------------------------------

User asks:

â€œIs climate change caused by humans?â€

Agent steps:

1.  Query DKG (via MCP query tool) for topic
    
2.  Retrieve notes (KA3)
    
3.  Apply corrections
    
4.  Respond with grounded truth
    
5.  Provide links to KAs for transparency
    

**ğŸ’¸ Flow 3: Premium Report via x402**
--------------------------------------

User/agent requests:

â€œGive me a full truth-consensus report.â€

*   402 Payment Required
    
*   Agent pays micro-fee
    
*   Server returns full report
    
*   Report includes:
    
    *   diff summary
        
    *   truth confidence
        
    *   claim-by-claim evidence
        
    *   weighted-reputation analysis
        

**ğŸ“Š 7. Data & Semantic Models**
================================

### **Core ontologies used:**

*   **schema.org**
    
*   **prov: (provenance ontology)**
    
*   **dkg: (DKG Knowledge Asset schema)**
    
*   **skos: for topic linking**
    
*   Optional: Wikidata QIDs
    

**ğŸ“š 8. Integration With Umanitek Guardian Knowledge Base**
===========================================================

Even though Challenge 1 doesnâ€™t require social graph, we use Guardian in **two advanced ways**:

### **1\. External Evidence**

If Grokipedia makes a controversial claim:â†’ Proof of whether that narrative circulates on X/Reddit (from Guardian data)

### **2\. Credibility Scoring**

Guardian provides metadata about misinformation clustersâ†’ Use that to discount biased claims

This gives TruthScope a **multi-source truth triangulation advantage**.